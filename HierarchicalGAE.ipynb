{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from math import ceil, floor\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv as GCNConv, dense_diff_pool\n",
    "from torch_geometric.nn import DenseSAGEConv\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, get_laplacian, remove_self_loops, to_dense_adj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes = 150\n",
    "\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "\n",
    "\n",
    "dataset = TUDataset('data', name='PROTEINS', transform=T.ToDense(max_nodes),\n",
    "                    pre_filter=MyFilter())\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "test_loader = DenseDataLoader(test_dataset, batch_size=20)\n",
    "val_loader = DenseDataLoader(val_dataset, batch_size=20)\n",
    "train_loader = DenseDataLoader(train_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GDN in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDNLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalization='sym', bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        \n",
    "        # in_channels and out_channels should be the same???\n",
    "        \n",
    "        # Learnable Parameters\n",
    "        self.W3 = Parameter(torch.randn(in_channels,in_channels))\n",
    "        self.W4 = Parameter(torch.randn(in_channels,in_channels))\n",
    "        self.W5 = Parameter(torch.randn(in_channels,out_channels))\n",
    "        \n",
    "    def __norm__(self, edge_index):\n",
    "        \n",
    "        edge_index, _ = remove_self_loops(edge_index=edge_index)\n",
    "        edge_index, _ = get_laplacian(edge_index=edge_index, normalization=self.normalization)\n",
    "        edge_index, _ = add_self_loops(edge_index=edge_index)\n",
    "        \n",
    "        return edge_index\n",
    "\n",
    "    def dense_to_sparse_with_attr(adj):\n",
    "        adj2 = adj.abs().sum(dim=-1)  \n",
    "        index = adj2.nonzero(as_tuple=True)\n",
    "        edge_attr = adj[index]\n",
    "        batch = index[0] * adj.size(-1)\n",
    "        index = (batch + index[1], batch + index[2])\n",
    "        edge_index = torch.stack(index, dim=0)\n",
    "        return edge_index, edge_attr\n",
    "        \n",
    "\n",
    "    def forward(self, batched_H, batched_A_):\n",
    "\n",
    "        batched_X_ = []\n",
    "        for i in range(len(batched_H)):\n",
    "            H = batched_H[i]\n",
    "            A_ = batched_A_[i]\n",
    "            # edge_index, _ = self.dense_to_sparse_with_attr(A_)\n",
    "            # edge_index, _ = self.__norm__(edge_index=edge_index)\n",
    "            # L_sym = to_dense_adj(edge_index=edge_index)\n",
    "            L_sym = A_\n",
    "            I_n = torch.eye(n=L_sym.shape[0])\n",
    "            order = 3\n",
    "            s = 3\n",
    "            eigendecomp_approx_list = [torch.linalg.matrix_power(L_sym, n) for n in range(1, order+1)] # list of L^n stored for future use\n",
    "\n",
    "            eigendecomp_L_approx = I_n\n",
    "            psi = I_n\n",
    "            psi_inverse = I_n\n",
    "            for i, L_sym_n in enumerate(eigendecomp_approx_list):\n",
    "                n = i + 1\n",
    "                eigendecomp_L_approx += L_sym_n                                             # Equation (9)\n",
    "                psi_magnitude = s**n / np.math.factorial(n)\n",
    "                psi += psi_magnitude if n % 2 == 0 else - psi_magnitude                     # Equation (11)\n",
    "                psi_inverse += psi_magnitude                                                # Equation (12)\n",
    "                \n",
    "            M = torch.sigmoid(eigendecomp_L_approx @ H @ self.W3)                           # Equation (10)\n",
    "\n",
    "            X_ = psi @ torch.relu(psi_inverse @ M @ self.W4) @ self.W5                      # Equation (13)\n",
    "            batched_X_.append(X_)\n",
    "            \n",
    "        batched_X_ = torch.stack(batched_X_)\n",
    "        print(batched_X_.shape)\n",
    "        return batched_X_\n",
    "\n",
    "\n",
    "class GDN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.GDN1 = GDNLayer(in_channels, hidden_channels)\n",
    "        self.GDN2 = GDNLayer(hidden_channels, hidden_channels)\n",
    "        self.GDN3 = GDNLayer(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, H, A_):\n",
    "        \n",
    "        X_ = self.GDN1(H, A_)\n",
    "        X_ = self.GDN2(X_, A_)\n",
    "        X_ = self.GDN3(X_, A_)\n",
    "\n",
    "        return X_\n",
    "    \n",
    "# class GDNLayer(MessagePassing):\n",
    "#     def __init__(self, in_channels, out_channels, normalization='sym', bias=True):\n",
    "#         super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.normalization = normalization\n",
    "        \n",
    "#         # in_channels and out_channels should be the same???\n",
    "        \n",
    "#         # Learnable Parameters\n",
    "#         self.W3 = Parameter(torch.tensor(in_channels,in_channels))\n",
    "#         self.W4 = Parameter(torch.tensor(in_channels,in_channels))\n",
    "#         self.W5 = Parameter(torch.tensor(in_channels,out_channels))\n",
    "\n",
    "        \n",
    "#     def __norm__(self, edge_index):\n",
    "        \n",
    "#         edge_index, _ = remove_self_loops(edge_index=edge_index)\n",
    "#         edge_index, _ = get_laplacian(edge_index=edge_index, normalization=self.normalization)\n",
    "#         edge_index, _ = add_self_loops(edge_index=edge_index)\n",
    "        \n",
    "#         return edge_index\n",
    "\n",
    "#     def forward(self, H, edge_index):\n",
    "#         edge_index, _ = self.__norm__(edge_index=edge_index)\n",
    "#         normalized_L = to_dense_adj(edge_index=edge_index)\n",
    "#         I_n = torch.eye(n=normalized_L.shape[0])\n",
    "#         order = 3\n",
    "#         s = 3\n",
    "#         eigendecomp_approx_list = [torch.linalg.matrix_power(normalized_L, n) for n in range(1, order+1)]\n",
    "#         eigendecomp_approx_list.insert(0, I_n)                                          # list of L^n stored for future use\n",
    "\n",
    "#         for n, L_n in enumerate(eigendecomp_approx_list):\n",
    "#             eigendecomp_L_approx += L_n                                                 # Equation (9)\n",
    "#             psi_magnitude = s**n / np.math.factorial(n)\n",
    "#             psi += psi_magnitude if n % 2 == 0 else - psi_magnitude                     # Equation (11)\n",
    "#             psi_inverse += psi_magnitude                                                # Equation (12)\n",
    "            \n",
    "#         M = torch.sigmoid(eigendecomp_L_approx @ H @ self.W3)                           # Equation (10)\n",
    "\n",
    "#         X_ = psi @ torch.relu(psi_inverse @ M @ self.W4) @ self.W5                      # Equation (13)\n",
    "        \n",
    "#         return X_\n",
    "\n",
    "        \n",
    "#     # def forward(self, x, edge_index):             # Standard GCN forward pass for reference\n",
    "#     #     # x has shape [N, in_channels]\n",
    "#     #     # edge_index has shape [2, E]\n",
    "\n",
    "#     #     # Step 1: Add self-loops to the adjacency matrix.\n",
    "#     #     edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "#     #     # Step 2: Linearly transform node feature matrix.\n",
    "#     #     x = self.lin(x)\n",
    "\n",
    "#     #     # Step 3: Compute normalization.\n",
    "#     #     row, col = edge_index\n",
    "#     #     deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "#     #     deg_inv_sqrt = deg.pow(-0.5)\n",
    "#     #     deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "#     #     norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "#     #     # Step 4-5: Start propagating messages.\n",
    "#     #     out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "#     #     # Step 6: Apply a final bias vector.\n",
    "#     #     out += self.bias\n",
    "\n",
    "#     #     return out\n",
    "\n",
    "#     # def message(self, x_j, norm):\n",
    "#     #     # x_j has shape [E, out_channels]\n",
    "\n",
    "#     #     # Step 4: Normalize node features.\n",
    "#     #     return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Other Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNSage(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if lin is True:\n",
    "            self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
    "                                       out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        # batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "        x0 = x\n",
    "        x1 = self.bn(1, self.conv1(x0, adj, mask).relu())\n",
    "        x2 = self.bn(2, self.conv2(x1, adj, mask).relu())\n",
    "        x3 = self.bn(3, self.conv3(x2, adj, mask).relu())\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        # self.bns = torch.nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, normalize))\n",
    "        # self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        self.convs.append(GCNConv(hidden_channels, hidden_channels, normalize))\n",
    "        # self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, normalize))\n",
    "        # self.bns.append(torch.nn.BatchNorm1d(out_channels))\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        batch_size, num_nodes, in_channels = x.size()\n",
    "        \n",
    "        for step in range(len(self.convs)):\n",
    "            # x = self.bns[step](F.relu(self.convs[step](x, adj, mask)))\n",
    "            x = F.relu(self.convs[step](x, adj, mask))\n",
    "\n",
    "        return x\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        num_nodes = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GNNSage(dataset.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = GNNSage(dataset.num_features, 64, 64, lin=False)\n",
    "\n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn2_pool = GNNSage(3 * 64, 64, num_nodes)\n",
    "        self.gnn2_embed = GNNSage(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = GNNSage(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(3 * 64, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HierarchicalGAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.s_list = []\n",
    "\n",
    "        # #----------------- Graph Convolution (Encoding layers) -----------------#\n",
    "        # num_nodes1 = ceil(0.25 * max_nodes)\n",
    "        # self.gnn1_pool = GNNSage(dataset.num_features, 64, num_nodes1)\n",
    "        # self.gnn1_embed = GNNSage(dataset.num_features, 64, 64, lin=False)\n",
    "\n",
    "        # num_nodes2 = ceil(0.25 * num_nodes1)\n",
    "        # self.gnn2_pool = GNNSage(3 * 64, 64, num_nodes2)\n",
    "        # self.gnn2_embed = GNNSage(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        # self.gnn3_embed = GNNSage(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "\n",
    "        # #----------------- Graph Deconvolution (Decoding layers) -----------------#\n",
    "\n",
    "        # self.gnn1_unpool = GNNSage(3 * 64, 64, num_nodes1)\n",
    "        # self.gdn1_embed_inv = GDN(num_nodes1, num_nodes1, num_nodes1)\n",
    "        \n",
    "        # self.gnn2_unpool = GNNSage(3 * 64, 64, 64, max_nodes)\n",
    "        # self.gdn2_embed_inv = GDN(max_nodes, max_nodes, max_nodes)\n",
    "        \n",
    "\n",
    "        #----------------- Graph Convolution (Encoding layers) -----------------#\n",
    "        num_nodes1 = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GCN(dataset.num_features, 64, num_nodes1)\n",
    "        self.gnn1_embed = GCN(dataset.num_features, 64, 64, lin=False)\n",
    "        \n",
    "        num_nodes2 = ceil(0.25 * num_nodes1)\n",
    "        self.gnn2_pool = GCN(64, 64, num_nodes2)\n",
    "        self.gnn2_embed = GCN(64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = GCN(64, 64, 64, lin=False)\n",
    "\n",
    "\n",
    "        #----------------- Graph Deconvolution (Decoding layers) -----------------#\n",
    "\n",
    "        self.gnn1_unpool = GCN(64, 64, 64, num_nodes1)\n",
    "        self.gdn1_embed_inv = GDN(64, 64, 64)\n",
    "        \n",
    "        self.gnn2_unpool = GCN(64, 64, 64, max_nodes)\n",
    "        self.gdn2_embed_inv = GDN(64, 64, 64)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def encode(self, x, adj, mask=None):                # DiffPool Hierarchical Encoding\n",
    "\n",
    "        s_0 = self.gnn1_pool(x, adj, mask)              # Learn Coarse Grained Mapping S_0 = GNN_pool(X, A)\n",
    "        z_0 = self.gnn1_embed(x, adj, mask)             # Learn First Layer Embeddings Z_0 = GNN_embed(X, A)\n",
    "        adj_0 = adj\n",
    "        self.s_list.append(s_0)       \n",
    "                \n",
    "        x_1 = s_0.transpose(1,2) @ z_0                  # Combining features from same communities  X_1 = S_0^T Z_0 \n",
    "        adj_1 = s_0.transpose(1,2) @ adj_0 @ s_0        # Use S_0 mapping to get coarse grained adj A_1 = S_0^T A_0 S_0 \n",
    "\n",
    "        s_1 = self.gnn2_pool(x_1, adj_1)                # Learn Coarser Grained Mapping S_1 = GNN_pool(X_1, A_1)\n",
    "        z_1 = self.gnn2_embed(x_1, adj_1)               # Learn Second Layer Embeddings Z_1 = GNN_embed(X_1, A_1)\n",
    "                \n",
    "        self.s_list.append(s_1)               \n",
    "        \n",
    "        x_2 = s_1.transpose(1,2) @ z_1                  # Combining features from same communities  X_2 = S_1^T Z_1 \n",
    "        adj_2 = s_1.transpose(1,2) @ adj_1 @ s_1        # Use S_1 mapping to get coarse grained adj A_2 = S_1^T A_1 S_1\n",
    "        \n",
    "        z_2 = self.gnn3_embed(x_2, adj_2)               # Learn Third Layer Embeddings  Z_2 = GNN_embed(X_2, A_2)\n",
    "        \n",
    "        # embedding = z_2.mean(dim=1)                     # Average remaining embeddings to generate whole graph embedding\n",
    "        # embedding = embedding.reshape(shape=(embedding.shape[0], 1, embedding.shape[1]))\n",
    "        \n",
    "        print(f\"Hierarchical Features Pooling: {z_0.shape} -> {z_1.shape} -> {z_2.shape}\")\n",
    "        print(f\"Hierarchical Adjacency Pooling: {adj_0.shape} -> {adj_1.shape} -> {adj_2.shape}\")\n",
    "        return z_2, adj_2\n",
    "    \n",
    "    \n",
    "    def decode(self, H, A, mask=None):\n",
    "        # s = self.gnn1_unpool(x, adj, mask)\n",
    "        # X_ = self.gdn1_embed_inv(X_, A)\n",
    "        \n",
    "        S = self.s_list[-1]\n",
    "        X1_ = S @ H                     \n",
    "        A1_ = S @ A @ S.transpose(1,2)\n",
    "        X1_ = self.gdn1_embed_inv(X1_, A1_)     #deconvolute these smoothed representations with GDN\n",
    "\n",
    "\n",
    "        S = self.s_list[-2]\n",
    "        X0_ = S @ X1_                      \n",
    "        A0_ = S @ A1_ @ S.transpose(1,2)\n",
    "        X0_ = self.gdn1_embed_inv(X0_, A0_)     #deconvolute these smoothed representations with GDN\n",
    "\n",
    "        X_ = X0_\n",
    "        A_ = A0_\n",
    "        # s = self.gnn2_unpool(x, adj, mask)\n",
    "        # x = self.gdn2_embed_inv(x, adj)\n",
    "        \n",
    "        # x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
    "        print(f\"Hierarchical Features Unpooling: {H.shape} -> {X1_.shape} -> {X0_.shape}\")\n",
    "        print(f\"Hierarchical Adjacency Unpooling: {A.shape} -> {A1_.shape} -> {A0_.shape}\")\n",
    "        return X_, A_\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x, adj = self.encode(x, adj, mask)\n",
    "        x, adj = self.decode(x, adj)\n",
    "        return x, adj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Features Pooling: torch.Size([20, 150, 64]) -> torch.Size([20, 38, 64]) -> torch.Size([20, 10, 64])\n",
      "Hierarchical Adjacency Pooling: torch.Size([20, 150, 150]) -> torch.Size([20, 38, 38]) -> torch.Size([20, 10, 10])\n",
      "torch.Size([20, 38, 64])\n",
      "torch.Size([20, 38, 64])\n",
      "torch.Size([20, 38, 64])\n",
      "torch.Size([20, 150, 64])\n",
      "torch.Size([20, 150, 64])\n",
      "torch.Size([20, 150, 64])\n",
      "Hierarchical Features Unpooling: torch.Size([20, 10, 64]) -> torch.Size([20, 38, 64]) -> torch.Size([20, 150, 64])\n",
      "Hierarchical Adjacency Unpooling: torch.Size([20, 10, 10]) -> torch.Size([20, 38, 38]) -> torch.Size([20, 150, 150])\n",
      "tensor([[[ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08],\n",
      "         [ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08],\n",
      "         [ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08],\n",
      "         ...,\n",
      "         [ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08],\n",
      "         [ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08],\n",
      "         [ 9.3804e+06,  5.2027e+07, -2.7934e+07,  ...,  1.2650e+08,\n",
      "           1.0753e+06,  1.0764e+08]],\n",
      "\n",
      "        [[ 5.5642e+06,  8.9123e+06, -2.4212e+07,  ...,  9.6926e+07,\n",
      "          -9.0078e+06,  9.7820e+07],\n",
      "         [ 5.5643e+06,  8.9124e+06, -2.4212e+07,  ...,  9.6928e+07,\n",
      "          -9.0079e+06,  9.7821e+07],\n",
      "         [ 5.5642e+06,  8.9124e+06, -2.4212e+07,  ...,  9.6927e+07,\n",
      "          -9.0079e+06,  9.7821e+07],\n",
      "         ...,\n",
      "         [ 5.5635e+06,  8.9112e+06, -2.4209e+07,  ...,  9.6914e+07,\n",
      "          -9.0067e+06,  9.7808e+07],\n",
      "         [ 5.5635e+06,  8.9112e+06, -2.4209e+07,  ...,  9.6914e+07,\n",
      "          -9.0067e+06,  9.7808e+07],\n",
      "         [ 5.5635e+06,  8.9112e+06, -2.4209e+07,  ...,  9.6914e+07,\n",
      "          -9.0067e+06,  9.7808e+07]],\n",
      "\n",
      "        [[ 3.6386e+06,  8.5883e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08],\n",
      "         [ 3.6386e+06,  8.5883e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08],\n",
      "         [ 3.6386e+06,  8.5883e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08],\n",
      "         ...,\n",
      "         [ 3.6385e+06,  8.5882e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08],\n",
      "         [ 3.6385e+06,  8.5882e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08],\n",
      "         [ 3.6385e+06,  8.5882e+07, -5.2888e+07,  ...,  1.4454e+08,\n",
      "           2.1559e+07,  1.1366e+08]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.9494e+07,  5.2444e+07, -1.8436e+07,  ...,  1.2032e+08,\n",
      "           1.8744e+07,  5.1053e+07],\n",
      "         [ 3.9516e+07,  5.2473e+07, -1.8446e+07,  ...,  1.2038e+08,\n",
      "           1.8754e+07,  5.1080e+07],\n",
      "         [ 3.9502e+07,  5.2454e+07, -1.8440e+07,  ...,  1.2034e+08,\n",
      "           1.8748e+07,  5.1062e+07],\n",
      "         ...,\n",
      "         [ 3.9433e+07,  5.2362e+07, -1.8407e+07,  ...,  1.2013e+08,\n",
      "           1.8715e+07,  5.0973e+07],\n",
      "         [ 3.9433e+07,  5.2362e+07, -1.8407e+07,  ...,  1.2013e+08,\n",
      "           1.8715e+07,  5.0973e+07],\n",
      "         [ 3.9433e+07,  5.2362e+07, -1.8407e+07,  ...,  1.2013e+08,\n",
      "           1.8715e+07,  5.0973e+07]],\n",
      "\n",
      "        [[-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08],\n",
      "         [-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08],\n",
      "         [-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08],\n",
      "         ...,\n",
      "         [-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08],\n",
      "         [-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08],\n",
      "         [-7.3258e+06,  6.2815e+07, -7.0622e+07,  ...,  7.0793e+07,\n",
      "           3.3864e+07,  1.1707e+08]],\n",
      "\n",
      "        [[ 8.2551e+06,  2.7707e+07, -4.7364e+07,  ...,  1.1996e+08,\n",
      "           4.3694e+07,  1.1768e+08],\n",
      "         [ 8.3560e+06,  2.8046e+07, -4.7942e+07,  ...,  1.2143e+08,\n",
      "           4.4228e+07,  1.1912e+08],\n",
      "         [ 8.3213e+06,  2.7929e+07, -4.7743e+07,  ...,  1.2092e+08,\n",
      "           4.4044e+07,  1.1862e+08],\n",
      "         ...,\n",
      "         [ 7.9664e+06,  2.6738e+07, -4.5707e+07,  ...,  1.1576e+08,\n",
      "           4.2166e+07,  1.1356e+08],\n",
      "         [ 7.9664e+06,  2.6738e+07, -4.5707e+07,  ...,  1.1576e+08,\n",
      "           4.2166e+07,  1.1356e+08],\n",
      "         [ 7.9664e+06,  2.6738e+07, -4.5707e+07,  ...,  1.1576e+08,\n",
      "           4.2166e+07,  1.1356e+08]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hgae = HierarchicalGAE().to(device)\n",
    "optimizer = torch.optim.Adam(hgae.parameters(), lr=0.001)\n",
    "\n",
    "hgae.train()\n",
    "for data in train_loader:\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = hgae(data.x, data.adj, data.mask)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DiffPool().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 151):\n",
    "    train_loss = train(epoch)\n",
    "    val_acc = test(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch_geometric\n",
    "\n",
    "for data in train_loader:\n",
    "    example = data.get_example(0)\n",
    "    print(example.edge_index)\n",
    "    g = torch_geometric.utils.to_networkx(example)\n",
    "    nx.draw(g)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('gaehta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f7820c31ebbde214c1b0e6d33454de7da2921c7bc231b433d2842e24018bb39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
