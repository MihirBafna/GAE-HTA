{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihirbafna/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from math import ceil, floor\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv as GCNConv, dense_diff_pool\n",
    "from torch_geometric.nn import DenseSAGEConv\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, get_laplacian, remove_self_loops, to_dense_adj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihirbafna/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DenseDataLoader' is deprecated, use 'loader.DenseDataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 150\n",
    "\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "\n",
    "\n",
    "dataset = TUDataset('data', name='PROTEINS', transform=T.ToDense(max_nodes),\n",
    "                    pre_filter=MyFilter())\n",
    "dataset = dataset.shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "test_loader = DenseDataLoader(test_dataset, batch_size=20)\n",
    "val_loader = DenseDataLoader(val_dataset, batch_size=20)\n",
    "train_loader = DenseDataLoader(train_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GDN in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, H, adj, mask=None):\n",
    "        edge_index, _ = remove_self_loops(edge_index=edge_index)\n",
    "        edge_index, _ = get_laplacian(edge_index=edge_index, normalization='sym')\n",
    "        edge_index, _ = add_self_loops(edge_index=edge_index)\n",
    "        edge_index, _ = self.__norm__(edge_index=edge_index)\n",
    "        normalized_L = to_dense_adj(edge_index=edge_index)\n",
    "        eigendecomp_L = np.identity(normalized_L.shape)\n",
    "        order = 3\n",
    "        for n in range(1, order+1):\n",
    "            eigendecomp_L += np.linalg.matrix_power(normalized_L, n)\n",
    "            \n",
    "        M = torch.sigmoid(eigendecomp_L @ H) # incorporate learnable parameter W3\n",
    "    \n",
    "\n",
    "\n",
    "class GDNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalization='sym', bias=True):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        \n",
    "        # in_channels and out_channels should be the same???\n",
    "        \n",
    "        # Learnable Parameters\n",
    "        self.W3 = Parameter(torch.tensor(in_channels,in_channels))\n",
    "        self.W4 = Parameter(torch.tensor(in_channels,in_channels))\n",
    "        self.W5 = Parameter(torch.tensor(in_channels,out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "        \n",
    "    def __norm__(self, edge_index):\n",
    "        \n",
    "        edge_index, _ = remove_self_loops(edge_index=edge_index)\n",
    "        edge_index, _ = get_laplacian(edge_index=edge_index, normalization=self.normalization)\n",
    "        edge_index, _ = add_self_loops(edge_index=edge_index)\n",
    "        \n",
    "        return edge_index\n",
    "\n",
    "    def forward(self, H, edge_index):\n",
    "        edge_index, _ = self.__norm__(edge_index=edge_index)\n",
    "        normalized_L = to_dense_adj(edge_index=edge_index)\n",
    "        I_n = torch.eye(n=normalized_L.shape[0])\n",
    "        order = 3\n",
    "        s = 3\n",
    "        eigendecomp_approx_list = [torch.linalg.matrix_power(normalized_L, n) for n in range(1, order+1)]\n",
    "        eigendecomp_approx_list.insert(0, I_n)                                          # list of L^n stored for future use\n",
    "\n",
    "        for n, L_n in enumerate(eigendecomp_approx_list):\n",
    "            eigendecomp_L_approx += L_n                                                 # Equation (9)\n",
    "            psi_magnitude = s**n / np.math.factorial(n)\n",
    "            psi += psi_magnitude if n % 2 == 0 else - psi_magnitude                     # Equation (11)\n",
    "            psi_inverse += psi_magnitude                                                # Equation (12)\n",
    "            \n",
    "        M = torch.sigmoid(eigendecomp_L_approx @ H @ self.W3)                           # Equation (10)\n",
    "\n",
    "        X_ = psi @ torch.relu(psi_inverse @ M @ self.W4) @ self.W5                      # Equation (13)\n",
    "        \n",
    "        return X_\n",
    "\n",
    "        \n",
    "    # def forward(self, x, edge_index):             # Standard GCN forward pass for reference\n",
    "    #     # x has shape [N, in_channels]\n",
    "    #     # edge_index has shape [2, E]\n",
    "\n",
    "    #     # Step 1: Add self-loops to the adjacency matrix.\n",
    "    #     edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "    #     # Step 2: Linearly transform node feature matrix.\n",
    "    #     x = self.lin(x)\n",
    "\n",
    "    #     # Step 3: Compute normalization.\n",
    "    #     row, col = edge_index\n",
    "    #     deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "    #     deg_inv_sqrt = deg.pow(-0.5)\n",
    "    #     deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    #     norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "    #     # Step 4-5: Start propagating messages.\n",
    "    #     out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    #     # Step 6: Apply a final bias vector.\n",
    "    #     out += self.bias\n",
    "\n",
    "    #     return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Other Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if lin is True:\n",
    "            self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
    "                                       out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        # batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "        x0 = x\n",
    "        x1 = self.bn(1, self.conv1(x0, adj, mask).relu())\n",
    "        x2 = self.bn(2, self.conv2(x1, adj, mask).relu())\n",
    "        x3 = self.bn(3, self.conv3(x2, adj, mask).relu())\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        num_nodes = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = GNN(dataset.num_features, 64, 64, lin=False)\n",
    "\n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn2_pool = GNN(3 * 64, 64, num_nodes)\n",
    "        self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(3 * 64, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "\n",
    "\n",
    "# Implement Graph Deconvolution Layer\n",
    "class GDN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "class HierarchicalGAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #----------------- Graph Convolution (Encoding layers) -----------------#\n",
    "        num_nodes1 = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes1)\n",
    "        self.gnn1_embed = GNN(dataset.num_features, 64, 64, lin=False)\n",
    "\n",
    "        num_nodes2 = ceil(0.25 * num_nodes1)\n",
    "        self.gnn2_pool = GNN(3 * 64, 64, num_nodes2)\n",
    "        self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "\n",
    "        #----------------- Graph Deconvolution (Decoding layers) -----------------#\n",
    "\n",
    "        self.gnn1_upsample = GNN(64, 64, 3 * 64, num_nodes1)\n",
    "        self.gnn1_embed_inverse = GNN(64, 64, 3 * 64,lin=False)\n",
    "        \n",
    "        self.gnn2_upsample = GNN(64, 64, 3 * 64, max_nodes)\n",
    "        self.gnn2_embed_inverse = GNN(64, 64, 3 * 64,lin=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def encode(self, x, adj, mask=None):                # DiffPool Hierarchical Encoding\n",
    "\n",
    "        s_0 = self.gnn1_pool(x, adj, mask)              # Learn Coarse Grained Mapping S_0 = GNN_pool(X, A)\n",
    "        z_0 = self.gnn1_embed(x, adj, mask)             # Learn First Layer Embeddings Z_0 = GNN_embed(X, A)\n",
    "        adj_0 = adj\n",
    "        \n",
    "        x_1 = s_0.transpose(1,2) @ z_0                  # Combining features from same communities  X_1 = S_0^T Z_0 \n",
    "        adj_1 = s_0.transpose(1,2) @ adj_0 @ s_0        # Use S_0 mapping to get coarse grained adj A_1 = S_0^T A_0 S_0 \n",
    "\n",
    "        s_1 = self.gnn2_pool(x_1, adj_1)                # Learn Coarser Grained Mapping S_1 = GNN_pool(X_1, A_1)\n",
    "        z_1 = self.gnn2_embed(x_1, adj_1)               # Learn Second Layer Embeddings Z_1 = GNN_embed(X_1, A_1)\n",
    "        \n",
    "        \n",
    "        x_2 = s_1.transpose(1,2) @ z_1                  # Combining features from same communities  X_2 = S_1^T Z_1 \n",
    "        adj_2 = s_1.transpose(1,2) @ adj_1 @ s_1        # Use S_1 mapping to get coarse grained adj A_2 = S_1^T A_1 S_1\n",
    "        \n",
    "        z_2 = self.gnn3_embed(x_2, adj_2)               # Learn Third Layer Embeddings  Z_2 = GNN_embed(X_2, A_2)\n",
    "        \n",
    "        embedding = z_2.mean(dim=1)                     # Average remaining embeddings to generate whole graph embedding\n",
    "        \n",
    "        return embedding, adj\n",
    "    \n",
    "    \n",
    "    def decode(self, x, adj, mask=None):\n",
    "        s = self.gnn1_upsample(x, adj, mask)\n",
    "        print(\"first upsample\")\n",
    "        x = self.gnn1_embed_inverse(x, adj, mask)\n",
    "        print(\"first deconvolution\")\n",
    "        x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
    "        \n",
    "        s = self.gnn2_upsample(x, adj, mask)\n",
    "        x = self.gnn2_embed_inverse(x, adj, mask)\n",
    "        \n",
    "        x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
    "\n",
    "        return x, adj\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x, adj = self.encode(x, adj, mask)\n",
    "        x, adj = self.decode(x, adj)\n",
    "        return x, adj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [20, 150] but got: [20, 20].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39msize())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=9'>10</a>\u001b[0m output, _ \u001b[39m=\u001b[39m hgae(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49madj, data\u001b[39m.\u001b[39;49mmask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=11'>12</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb Cell 3\u001b[0m in \u001b[0;36mHierarchicalGAE.forward\u001b[0;34m(self, x, adj, mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=173'>174</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=174'>175</a>\u001b[0m     x, adj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(x, adj, mask)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=175'>176</a>\u001b[0m     x, adj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(x, adj)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=176'>177</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x, adj\n",
      "\u001b[1;32m/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb Cell 3\u001b[0m in \u001b[0;36mHierarchicalGAE.decode\u001b[0;34m(self, x, adj, mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=159'>160</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, x, adj, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=160'>161</a>\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgnn1_upsample(x, adj, mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=161'>162</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfirst upsample\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=162'>163</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgnn1_embed_inverse(x, adj, mask)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb Cell 3\u001b[0m in \u001b[0;36mGNN.forward\u001b[0;34m(self, x, adj, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=47'>48</a>\u001b[0m     \u001b[39m# batch_size, num_nodes, in_channels = x.size()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=49'>50</a>\u001b[0m     x0 \u001b[39m=\u001b[39m x\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=50'>51</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x0, adj, mask)\u001b[39m.\u001b[39mrelu())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=51'>52</a>\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x1, adj, mask)\u001b[39m.\u001b[39mrelu())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihirbafna/Documents/Research/MaLab/GAE-HTA/GAE-HTA.ipynb#ch0000002?line=52'>53</a>\u001b[0m     x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x2, adj, mask)\u001b[39m.\u001b[39mrelu())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gaehta/lib/python3.8/site-packages/torch_geometric/nn/dense/dense_sage_conv.py:52\u001b[0m, in \u001b[0;36mDenseSAGEConv.forward\u001b[0;34m(self, x, adj, mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m adj \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m adj\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m adj\n\u001b[1;32m     50\u001b[0m B, N, _ \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 52\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(adj, x)\n\u001b[1;32m     53\u001b[0m out \u001b[39m=\u001b[39m out \u001b[39m/\u001b[39m adj\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_rel(out) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_root(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [20, 150] but got: [20, 20]."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hgae = HierarchicalGAE().to(device)\n",
    "optimizer = torch.optim.Adam(hgae.parameters(), lr=0.001)\n",
    "\n",
    "hgae.train()\n",
    "for data in train_loader:\n",
    "    data = data.to(device)\n",
    "    print(data.size())\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = hgae(data.x, data.adj, data.mask)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 151):\n",
    "    train_loss = train(epoch)\n",
    "    val_acc = test(val_loader)\n",
    "    if val_acc > best_val_acc:\n",
    "        test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch_geometric\n",
    "\n",
    "for data in train_loader:\n",
    "    example = data.get_example(0)\n",
    "    print(example.edge_index)\n",
    "    g = torch_geometric.utils.to_networkx(example)\n",
    "    nx.draw(g)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('gaehta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f7820c31ebbde214c1b0e6d33454de7da2921c7bc231b433d2842e24018bb39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
